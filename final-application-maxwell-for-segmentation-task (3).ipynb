{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7040769,"sourceType":"datasetVersion","datasetId":4020939},{"sourceId":8221096,"sourceType":"datasetVersion","datasetId":4873800}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ePURE","metadata":{}},{"cell_type":"code","source":"# --- ePURE Implementation (Provided) ---\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ConvBlock(nn.Module):\n    \"\"\"Một khối tích chập cơ bản: Conv -> BatchNorm -> ReLU\"\"\"\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n    def forward(self, x):\n        return self.block(x)\n\nclass SEBlock(nn.Module):\n    \"\"\"Khối Squeeze-and-Excitation cho Channel Attention\"\"\"\n    def __init__(self, channels, reduction_ratio=16):\n        super().__init__()\n        self.squeeze = nn.AdaptiveAvgPool2d(1)\n        self.excitation = nn.Sequential(\n            nn.Linear(channels, channels // reduction_ratio, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channels // reduction_ratio, channels, bias=False),\n            nn.Sigmoid()\n        )\n    def forward(self, x):\n        b, c, _, _ = x.shape\n        y = self.squeeze(x).view(b, c)\n        y = self.excitation(y).view(b, c, 1, 1)\n        return x * y.expand_as(x)\n\nclass ePURE(nn.Module):\n    \"\"\"\n    Phiên bản ePURE hoàn chỉnh nhất với:\n    - Mạng sâu hơn (Deeper).\n    - Lớp BatchNorm2d (Normalization).\n    - Kết nối tắt (Residual Connection).\n    - Cơ chế chú ý (Attention).\n    \"\"\"\n    def __init__(self, in_channels, base_channels=32):\n        super().__init__()\n        # Các khối tích chập\n        self.block1 = ConvBlock(in_channels, base_channels)\n        self.block2 = ConvBlock(base_channels, base_channels)\n        \n        # THÊM MỚI: Khối Attention\n        self.attention = SEBlock(channels=base_channels)\n        \n        self.block3 = ConvBlock(base_channels, base_channels)\n        self.final_conv = nn.Conv2d(base_channels, 1, kernel_size=1)\n\n    def forward(self, x):\n        x_float = x.float()\n\n        # Luồng dữ liệu qua các khối\n        out_block1 = self.block1(x_float)\n        out_block2 = self.block2(out_block1)\n        \n        # Áp dụng kết nối tắt\n        residual_out = out_block2 + out_block1\n        \n        # Áp dụng Attention\n        attention_out = self.attention(residual_out)\n        \n        # Đi qua khối cuối cùng\n        out_block3 = self.block3(attention_out)\n        \n        # Tạo bản đồ nhiễu cuối cùng\n        noise_map = self.final_conv(out_block3)\n        \n        return noise_map","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T19:55:03.467176Z","iopub.execute_input":"2025-10-07T19:55:03.467370Z","iopub.status.idle":"2025-10-07T19:55:07.966319Z","shell.execute_reply.started":"2025-10-07T19:55:03.467350Z","shell.execute_reply":"2025-10-07T19:55:07.965511Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Code để tính và in tham số ---\n\n# 1. Khởi tạo mô hình\n# Giả sử đầu vào là ảnh RGB (3 kênh) và base_channels=32\nmodel = ePURE(in_channels=3, base_channels=32)\n\n# 2. Tính tổng số tham số có thể huấn luyện\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"Tổng số tham số có thể huấn luyện của khối ePURE: {total_params:,}\")\n\n# (Tùy chọn) In chi tiết tham số của từng lớp\nprint(\"\\n--- Chi tiết tham số của từng lớp ---\")\nfor name, parameter in model.named_parameters():\n    if parameter.requires_grad:\n        print(f\"{name:<40} | Số tham số: {parameter.numel():,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T19:55:07.968148Z","iopub.execute_input":"2025-10-07T19:55:07.968445Z","iopub.status.idle":"2025-10-07T19:55:08.016955Z","shell.execute_reply.started":"2025-10-07T19:55:07.968427Z","shell.execute_reply":"2025-10-07T19:55:08.016233Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Adaptive_Smoothing_Function","metadata":{}},{"cell_type":"code","source":"import torchvision.transforms.functional as TF\n# --- Adaptive Smoothing Implementation (Provided) ---\ndef adaptive_smoothing(x, noise_profile, kernel_size=5, sigma=1.0):\n    \"\"\"\n    Áp dụng làm mịn thích nghi dựa trên noise_profile\n    - x: Ảnh đầu vào hoặc feature map [B, C, H, W]\n    - noise_profile: Bản đồ nhiễu [B, 1, H, W] (giá trị từ 0 đến 1)\n    - kernel_size/sigma: Tham số làm mịn Gaussian\n    \"\"\"\n    # Ensure input is float for convolution\n    x_float = x.float()\n\n    # Ensure noise_profile is float and 1 channel\n    noise_profile_float = noise_profile.float()\n    if noise_profile_float.size(1) != 1:\n         print(f\"Warning: Noise profile expected 1 channel but got {noise_profile_float.size(1)}. Using first channel.\")\n         noise_profile_float = noise_profile_float[:, :1, :, :]\n\n    # Bước 1: Apply Gaussian blur channel-wise\n    if isinstance(kernel_size, int):\n        kernel_size_tuple = (kernel_size, kernel_size)\n    else:\n        kernel_size_tuple = kernel_size\n\n    if isinstance(sigma, (int, float)):\n         sigma_tuple = (float(sigma), float(sigma))\n    else:\n         sigma_tuple = sigma\n\n    # Ensure sigma values are positive to avoid issues\n    sigma_tuple = tuple(max(0.1, s) for s in sigma_tuple) # Add small epsilon\n\n    smoothed = TF.gaussian_blur(x_float, kernel_size=kernel_size_tuple, sigma=sigma_tuple)\n\n    # Bước 2: Chuẩn hóa noise_profile (sigmoid) và mở rộng cho đúng số kênh\n    # Sigmoid ensures blending weights are between 0 and 1\n    # A higher noise_profile value should lead to *more* smoothing.\n    # So, blending_weights = noise_profile (after sigmoid)\n    blending_weights = torch.sigmoid(noise_profile_float) # [B, 1, H, W]\n\n    # Expand blending_weights to match the number of channels in x\n    blending_weights = blending_weights.repeat(1, x_float.size(1), 1, 1) # [B, C, H, W]\n\n    # Ensure dimensions match for blending\n    assert blending_weights.shape == x_float.shape, f\"Blending weights shape {blending_weights.shape} does not match input shape {x_float.shape}\"\n\n    # Bước 3: Trộn ảnh gốc và ảnh đã làm mịn\n    # Output = (1 - alpha) * Original + alpha * Smoothed\n    # where alpha = blending_weights\n    weighted_sum = x_float * (1 - blending_weights) + smoothed * blending_weights\n\n    return weighted_sum","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T19:55:08.017780Z","iopub.execute_input":"2025-10-07T19:55:08.018091Z","iopub.status.idle":"2025-10-07T19:55:10.996003Z","shell.execute_reply.started":"2025-10-07T19:55:08.018061Z","shell.execute_reply":"2025-10-07T19:55:10.995384Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Adaptive_Quantum_Noise_Injection","metadata":{}},{"cell_type":"code","source":"def adaptive_quantum_noise_injection(\n    features, \n    noise_map, \n    T_min=0.5, \n    T_max=1.5, \n    pauli_prob={'X': 0.00096, 'Y': 0.00096, 'Z': 0.00096}\n):\n    \"\"\"\n    Áp dụng nhiễu lượng tử một cách THÍCH NGHI dựa trên noise_map.\n    - Nơi noise_map thấp (vùng sạch), T sẽ cao -> thêm nhiều nhiễu.\n    - Nơi noise_map cao (vùng nhiễu), T sẽ thấp -> thêm ít nhiễu.\n    \n    Args:\n        features (torch.Tensor): Tensor đầu vào [B, C, H, W].\n        noise_map (torch.Tensor): Bản đồ nhiễu từ ePURE [B, 1, H, W].\n        T_min (float): Hệ số nhiễu tối thiểu.\n        T_max (float): Hệ số nhiễu tối đa.\n        pauli_prob (dict): Xác suất cơ sở của các cổng Pauli.\n    \"\"\"\n    features_float = features.float()\n    noise_map_float = noise_map.float()\n    device = features_float.device\n\n    # Bước 1: Tạo bản đồ hệ số nhiễu T (T_map) từ noise_map\n    # Dùng sigmoid để chuẩn hóa noise_map về [0, 1]\n    # Ta muốn T cao khi noise_map thấp, nên ta dùng (1 - sigmoid)\n    normalized_noise = torch.sigmoid(noise_map_float)\n    T_map = T_max - (T_max - T_min) * normalized_noise # Ánh xạ ngược: noise thấp -> T cao\n    T_map = T_map.repeat(1, features.size(1), 1, 1) # Mở rộng cho các kênh\n\n    # Bước 2: Tính toán xác suất Pauli theo không gian (spatially-varying probabilities)\n    p_x = pauli_prob['X'] * T_map\n    p_y = pauli_prob['Y'] * T_map\n    p_z = pauli_prob['Z'] * T_map\n    p_none = 1.0 - (p_x + p_y + p_z)\n    \n    # [B, C, H, W, 4] -> stack các xác suất\n    probabilities_map = torch.stack([p_x, p_y, p_z, p_none], dim=-1)\n    \n    # Bước 3: Lấy mẫu cổng Pauli cho từng pixel\n    # Reshape để dùng multinomial\n    B, C, H, W = features.shape\n    prob_reshaped = probabilities_map.view(-1, 4)\n    choice_indices = torch.multinomial(prob_reshaped, 1).view(B, C, H, W)\n    \n    # Bước 4: Áp dụng nhiễu dựa trên lựa chọn\n    noisy_features = features_float.clone()\n    \n    # Mask cho từng cổng\n    mask_x = (choice_indices == 0)\n    mask_y = (choice_indices == 1)\n    mask_z = (choice_indices == 2)\n    \n    # Áp dụng cổng Pauli\n    noisy_features[mask_x] = 1.0 - noisy_features[mask_x]\n    noisy_features[mask_y] = 1.0 - noisy_features[mask_y] + 0.1 * torch.randn_like(noisy_features[mask_y])\n    noisy_features[mask_z] = -noisy_features[mask_z]\n    \n    # Đảm bảo giá trị pixel nằm trong phạm vi hợp lệ\n    noisy_features = torch.clamp(noisy_features, 0.0, 1.0)\n    \n    return noisy_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T19:55:10.996660Z","iopub.execute_input":"2025-10-07T19:55:10.996960Z","iopub.status.idle":"2025-10-07T19:55:11.004251Z","shell.execute_reply.started":"2025-10-07T19:55:10.996942Z","shell.execute_reply":"2025-10-07T19:55:11.003476Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Funt to get B1 map","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nimport numpy as np\nimport torchvision.transforms.functional as TF\nfrom scipy.ndimage import binary_fill_holes, binary_opening\n\nclass AdvancedB1Simulator(nn.Module):\n    \"\"\"\n    Mô phỏng B1 map dựa trên một mảng các cuộn dây bề mặt (surface coils) ngẫu nhiên.\n    Cung cấp B1 map chân thực hơn mà vẫn nhẹ và hiệu quả.\n    \"\"\"\n    def __init__(self,\n                 n_coils_range: tuple = (4, 8),\n                 strength_range: tuple = (0.5, 1.5),\n                 radius_factor_range: tuple = (0.5, 1.5)):\n        super().__init__()\n        self.n_coils_range = n_coils_range\n        self.strength_range = strength_range\n        self.radius_factor_range = radius_factor_range\n\n    def forward(self, image_batch: torch.Tensor) -> torch.Tensor:\n        batch_size, _, height, width = image_batch.shape\n        device = image_batch.device\n\n        b1_maps = []\n        for i in range(batch_size):\n            # 1. Ngẫu nhiên hóa các tham số của mảng coil\n            n_coils = torch.randint(self.n_coils_range[0], self.n_coils_range[1] + 1, (1,)).item()\n            \n            centers_x = torch.randint(-width//4, width + width//4, (n_coils,), device=device)\n            centers_y = torch.randint(-height//4, height + height//4, (n_coils,), device=device)\n            \n            strengths = torch.zeros(n_coils, device=device).uniform_(*self.strength_range)\n            base_radius = (height + width) / 4\n            radii = torch.zeros(n_coils, device=device).uniform_(*self.radius_factor_range) * base_radius\n\n            # 2. Tạo bản đồ độ nhạy cho từng coil\n            y_coords = torch.arange(height, device=device)\n            x_coords = torch.arange(width, device=device)\n            y_grid, x_grid = torch.meshgrid(y_coords, x_coords, indexing='ij')\n\n            coil_maps = []\n            for j in range(n_coils):\n                dist_sq = (x_grid - centers_x[j])**2 + (y_grid - centers_y[j])**2\n                sensitivity_map = strengths[j] / (dist_sq + radii[j]**2)\n                coil_maps.append(sensitivity_map)\n            \n            coil_maps = torch.stack(coil_maps, dim=0)\n\n            # 3. Kết hợp các coil map bằng phương pháp \"sum of squares\"\n            combined_map = torch.sqrt(torch.sum(coil_maps**2, dim=0))\n            \n            # Chuẩn hóa để có giá trị trung bình gần 1\n            combined_map = combined_map / (torch.mean(combined_map) + 1e-8)\n            \n            b1_maps.append(combined_map)\n\n        b1_map_stack = torch.stack(b1_maps, dim=0).unsqueeze(1)\n\n        # Clip về dải giá trị vật lý hợp lý\n        b1_map_stack = torch.clamp(b1_map_stack, 0.4, 1.6)\n\n        return b1_map_stack\n\ndef calculate_ultimate_common_b1_map(\n    all_images: torch.Tensor,\n    device: str = 'cuda',\n    save_path: str = \"ultimate_common_b1_map.pth\"\n) -> torch.Tensor:\n    \"\"\"\n    Tính toán một B1 map chung với độ chính xác cao nhất bằng cách kết hợp:\n    1. Mô phỏng coil-array (AdvancedB1Simulator).\n    2. Trung bình có trọng số theo chất lượng ảnh và vùng quan tâm (ROI).\n    3. Hậu xử lý làm mịn.\n    \"\"\"\n    calc_device = torch.device(device if torch.cuda.is_available() else 'cpu')\n\n    if os.path.exists(save_path):\n        print(f\"Đang tải Ultimate B1 map đã được tính toán từ '{save_path}'...\")\n        saved_data = torch.load(save_path, map_location=calc_device)\n        return saved_data['common_b1_map']\n\n    print(\"Bắt đầu tính toán Ultimate B1 map mới...\")\n    \n    # Bước 1: Tạo các B1 map chất lượng cao\n    b1_simulator = AdvancedB1Simulator().to(calc_device)\n    num_images = all_images.shape[0]\n    batch_size = 32\n    \n    all_generated_maps = []\n    all_image_stats = []\n\n    print(\"Tạo các B1 map ngẫu nhiên (chất lượng cao)...\")\n    with torch.no_grad():\n        for i in range(0, num_images, batch_size):\n            end_idx = min(i + batch_size, num_images)\n            batch_images = all_images[i:end_idx].to(calc_device)\n            \n            generated_maps = b1_simulator(batch_images)\n            all_generated_maps.append(generated_maps.cpu())\n\n            for j in range(batch_images.shape[0]):\n                img = batch_images[j].cpu()\n                all_image_stats.append({\n                    'mean': torch.mean(img).item(),\n                    'std': torch.std(img).item()\n                })\n\n    all_generated_maps = torch.cat(all_generated_maps, dim=0)\n\n    # Bước 2: Tạo các trọng số cho việc tính trung bình\n    print(\"Tạo trọng số để tính trung bình...\")\n    \n    # a. Trọng số theo chất lượng ảnh (ưu tiên ảnh có độ tương phản cao)\n    image_weights = []\n    for stats in all_image_stats:\n        contrast_score = stats['std'] / (stats['mean'] + 1e-8) if stats['mean'] > 0 else 0\n        weight = np.clip(contrast_score, 0.5, 2.0)\n        image_weights.append(weight)\n    image_weights = torch.tensor(image_weights, dtype=torch.float32).view(-1, 1, 1, 1)\n\n    # b. Trọng số theo vùng không gian (ưu tiên vùng giải phẫu)\n    avg_image = torch.mean(all_images, dim=0).squeeze().numpy()\n    roi_mask_np = avg_image > np.mean(avg_image) * 0.5\n    roi_mask_np = binary_opening(roi_mask_np, structure=np.ones((5,5)))\n    roi_mask_np = binary_fill_holes(roi_mask_np)\n    roi_mask = torch.from_numpy(roi_mask_np.astype(np.float32)).unsqueeze(0).unsqueeze(0)\n    \n    spatial_weights = torch.ones_like(roi_mask)\n    spatial_weights[roi_mask == 1] = 3.0 # Vùng giải phẫu quan trọng gấp 3 lần\n\n    # Bước 3: Tính trung bình có trọng số\n    print(\"Tính toán trung bình có trọng số...\")\n    weighted_maps = all_generated_maps * image_weights * spatial_weights\n    total_weights = image_weights * spatial_weights\n    \n    common_b1_map = torch.sum(weighted_maps, dim=0, keepdim=True) / (torch.sum(total_weights, dim=0, keepdim=True) + 1e-8)\n\n    # Bước 4: Hậu xử lý làm mịn\n    print(\"Hậu xử lý làm mịn B1 map...\")\n    common_b1_map = TF.gaussian_blur(common_b1_map, kernel_size=21, sigma=5)\n    \n    # Chuẩn hóa lại để giá trị trung bình gần 1\n    common_b1_map = common_b1_map / (torch.mean(common_b1_map) + 1e-8)\n    common_b1_map = torch.clamp(common_b1_map, 0.5, 1.5)\n\n    print(f\"Lưu Ultimate B1 map vào '{save_path}'...\")\n    torch.save({'common_b1_map': common_b1_map}, save_path)\n    \n    print(\"Tính toán Ultimate B1 map thành công!\")\n    return common_b1_map.to(calc_device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T19:55:11.005009Z","iopub.execute_input":"2025-10-07T19:55:11.005223Z","iopub.status.idle":"2025-10-07T19:55:11.264791Z","shell.execute_reply.started":"2025-10-07T19:55:11.005192Z","shell.execute_reply":"2025-10-07T19:55:11.264171Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Maxell Solver","metadata":{}},{"cell_type":"code","source":"# --- CÁC HÀM VẬT LÝ ĐỘC LẬP ---\n\ndef _laplacian_2d(x_complex):\n    \"\"\"Tính toán toán tử Laplace 2D cho một tensor phức.\"\"\"\n    k = torch.tensor([[0., 1., 0.], [1., -4., 1.], [0., 1., 0.]], \n                     device=x_complex.device).reshape(1, 1, 3, 3)\n    \n    groups_real = x_complex.real.size(1) if x_complex.real.size(1) > 0 else 1\n    groups_imag = x_complex.imag.size(1) if x_complex.imag.size(1) > 0 else 1\n    \n    real_lap = F.conv2d(x_complex.real, k.repeat(groups_real, 1, 1, 1), padding=1, groups=groups_real)\n    imag_lap = F.conv2d(x_complex.imag, k.repeat(groups_imag, 1, 1, 1), padding=1, groups=groups_imag)\n    \n    return torch.complex(real_lap, imag_lap)\n\ndef compute_helmholtz_residual(b1_map, eps, sigma, k0):\n    \"\"\"Tính toán phần dư của phương trình Helmholtz.\"\"\"\n    k0 = k0.to(b1_map.device)\n    omega = 2 * np.pi * 42.58e6\n    \n    b1_map_complex = torch.complex(b1_map, torch.zeros_like(b1_map)) if not b1_map.is_complex() else b1_map\n    \n    eps_r, sig_r = eps.to(b1_map_complex.device), sigma.to(b1_map_complex.device)\n    \n    size = b1_map_complex.shape[2:]\n    up_eps = F.interpolate(eps_r, size=size, mode='bilinear', align_corners=False)\n    up_sig = F.interpolate(sig_r, size=size, mode='bilinear', align_corners=False)\n    \n    eps_c = torch.complex(up_eps, -up_sig / omega)\n    lap_b1 = _laplacian_2d(b1_map_complex)\n    \n    res = lap_b1 + (k0 ** 2) * eps_c * b1_map_complex\n    return res.real ** 2 + res.imag ** 2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T19:55:11.265322Z","iopub.execute_input":"2025-10-07T19:55:11.265623Z","iopub.status.idle":"2025-10-07T19:55:11.275276Z","shell.execute_reply.started":"2025-10-07T19:55:11.265604Z","shell.execute_reply":"2025-10-07T19:55:11.274722Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Lớp MaxwellSolver đã được đơn giản hóa\nclass MaxwellSolver(nn.Module):\n    def __init__(self, in_channels, hidden_dim=32):\n        super(MaxwellSolver, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv2d(in_channels, hidden_dim, kernel_size=3, padding=1), \n            nn.ReLU(),\n            nn.Conv2d(hidden_dim, 2, kernel_size=3, padding=1)\n        )\n\n    def forward(self, x):\n        eps_sigma_map = self.encoder(x)\n        return eps_sigma_map[:, 0:1, :, :], eps_sigma_map[:, 1:2, :, :]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T19:55:11.277481Z","iopub.execute_input":"2025-10-07T19:55:11.277674Z","iopub.status.idle":"2025-10-07T19:55:11.293736Z","shell.execute_reply.started":"2025-10-07T19:55:11.277657Z","shell.execute_reply":"2025-10-07T19:55:11.292984Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Robust Med Physics Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ASPPConv(nn.Sequential):\n    # Một khối tích chập cơ bản cho các nhánh của ASPP\n    def __init__(self, in_channels, out_channels, dilation):\n        super(ASPPConv, self).__init__(\n            nn.Conv2d(in_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU()\n        )\n\nclass ASPPPooling(nn.Sequential):\n    # Nhánh global average pooling\n    def __init__(self, in_channels, out_channels):\n        super(ASPPPooling, self).__init__(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU()\n        )\n\n    def forward(self, x):\n        size = x.shape[-2:]\n        x = super(ASPPPooling, self).forward(x)\n        return F.interpolate(x, size=size, mode='bilinear', align_corners=False)\n\nclass BottleneckASPP(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(BottleneckASPP, self).__init__()\n        # Các rate này phù hợp cho ảnh kích thước 224x224, feature map ở bottleneck ~14x14\n        atrous_rates = [3, 6, 9] \n        \n        # Số kênh đầu ra cho mỗi nhánh con, sau đó sẽ được gộp lại\n        inter_channels = out_channels // (len(atrous_rates) + 2) # Chia cho 5 nhánh\n        \n        self.convs = nn.ModuleList([\n            # Nhánh 1x1 conv\n            nn.Sequential(\n                nn.Conv2d(in_channels, inter_channels, 1, bias=False),\n                nn.BatchNorm2d(inter_channels),\n                nn.ReLU()\n            ),\n            # Các nhánh dilated conv\n            ASPPConv(in_channels, inter_channels, atrous_rates[0]),\n            ASPPConv(in_channels, inter_channels, atrous_rates[1]),\n            ASPPConv(in_channels, inter_channels, atrous_rates[2]),\n            # Nhánh pooling\n            ASPPPooling(in_channels, inter_channels)\n        ])\n\n        # Lớp tích chập cuối cùng để gộp các đặc trưng\n        self.project = nn.Sequential(\n            nn.Conv2d(inter_channels * 5, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n            nn.Dropout(0.5) # Thêm Dropout để chống overfitting\n        )\n\n    def forward(self, x):\n        res = [conv(x) for conv in self.convs]\n        res = torch.cat(res, dim=1)\n        return self.project(res)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T19:55:11.295760Z","iopub.execute_input":"2025-10-07T19:55:11.297439Z","iopub.status.idle":"2025-10-07T19:55:11.312296Z","shell.execute_reply.started":"2025-10-07T19:55:11.297414Z","shell.execute_reply":"2025-10-07T19:55:11.311519Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1. Basic Conv Block","metadata":{}},{"cell_type":"code","source":"# --- Standard Convolutional Block ---\nclass BasicConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, use_bn=True):\n        super().__init__()\n        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=not use_bn)]\n        if use_bn:\n            layers.append(nn.BatchNorm2d(out_channels))\n        layers.append(nn.ReLU(inplace=True))\n        self.block = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.block(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T19:55:11.313050Z","iopub.execute_input":"2025-10-07T19:55:11.313301Z","iopub.status.idle":"2025-10-07T19:55:11.329491Z","shell.execute_reply.started":"2025-10-07T19:55:11.313280Z","shell.execute_reply":"2025-10-07T19:55:11.328895Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"2. Encoder Block","metadata":{}},{"cell_type":"code","source":"# --- Model Components (U-Net based) ---\nclass EncoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv_block1 = BasicConvBlock(in_channels, out_channels)\n        self.conv_block2 = BasicConvBlock(out_channels, out_channels)\n        self.noise_estimator = ePURE(in_channels=in_channels)\n\n    def forward(self, x):\n        noise_profile = self.noise_estimator(x)\n        x_smoothed = adaptive_smoothing(x, noise_profile)\n        x = self.conv_block1(x_smoothed)\n        x = self.conv_block2(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T19:55:11.330159Z","iopub.execute_input":"2025-10-07T19:55:11.330347Z","iopub.status.idle":"2025-10-07T19:55:11.346888Z","shell.execute_reply.started":"2025-10-07T19:55:11.330333Z","shell.execute_reply":"2025-10-07T19:55:11.346266Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"3. Decoder Block","metadata":{}},{"cell_type":"code","source":"class DecoderBlock(nn.Module):\n    def __init__(self, in_channels, skip_channels, out_channels):\n        super().__init__()\n        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n        concat_ch = in_channels // 2 + skip_channels\n        self.maxwell_solver = MaxwellSolver(concat_ch)\n        self.conv_block1 = BasicConvBlock(concat_ch, out_channels)\n        self.conv_block2 = BasicConvBlock(out_channels, out_channels)\n\n    def forward(self, x, skip_connection):\n        x = self.up(x)\n        diffY, diffX = skip_connection.size()[2]-x.size()[2], skip_connection.size()[3]-x.size()[3]\n        x = F.pad(x, [diffX//2, diffX-diffX//2, diffY//2, diffY-diffY//2])\n        x_cat = torch.cat([skip_connection, x], dim=1)\n        es_tuple = self.maxwell_solver(x_cat)\n        out = self.conv_block1(x_cat)\n        out = self.conv_block2(out)\n        return out, es_tuple","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T19:55:11.347698Z","iopub.execute_input":"2025-10-07T19:55:11.347963Z","iopub.status.idle":"2025-10-07T19:55:11.366478Z","shell.execute_reply.started":"2025-10-07T19:55:11.347946Z","shell.execute_reply":"2025-10-07T19:55:11.365512Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"5. Model","metadata":{}},{"cell_type":"code","source":"class RobustMedVFL_UNet(nn.Module):\n    \"\"\"\n    Kiến trúc UNet++ tích hợp các khối Encoder/Decoder tùy chỉnh (RobustMedVFL).\n    Hỗ trợ deep supervision.\n    \"\"\"\n    def __init__(self, n_channels=1, n_classes=4, deep_supervision=True):\n        super().__init__()\n        self.deep_supervision = deep_supervision\n        \n        # --- Các kênh đặc trưng ở mỗi tầng ---\n        channels = [16, 32, 64, 128, 256]\n\n        # --- Encoder (Cột j=0) ---\n        # Sử dụng EncoderBlock tùy chỉnh của bạn\n        self.conv0_0 = EncoderBlock(n_channels, channels[0])\n        self.conv1_0 = EncoderBlock(channels[0], channels[1])\n        self.conv2_0 = EncoderBlock(channels[1], channels[2])\n        self.conv3_0 = EncoderBlock(channels[2], channels[3])\n        self.conv4_0 = BottleneckASPP(channels[3], channels[4]) # Bottleneck\n\n\n        # --- Lớp Pooling ---\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        # --- Lớp Upsampling ---\n        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n\n        # --- Các khối trên kết nối tắt (j > 0) ---\n        # Cột j=1\n        self.conv0_1 = BasicConvBlock(channels[0] + channels[1], channels[0])\n        self.conv1_1 = BasicConvBlock(channels[1] + channels[2], channels[1])\n        self.conv2_1 = BasicConvBlock(channels[2] + channels[3], channels[2])\n        self.conv3_1 = BasicConvBlock(channels[3] + channels[4], channels[3])\n\n        # Cột j=2\n        self.conv0_2 = BasicConvBlock(channels[0]*2 + channels[1], channels[0])\n        self.conv1_2 = BasicConvBlock(channels[1]*2 + channels[2], channels[1])\n        self.conv2_2 = BasicConvBlock(channels[2]*2 + channels[3], channels[2])\n\n        # Cột j=3\n        self.conv0_3 = BasicConvBlock(channels[0]*3 + channels[1], channels[0])\n        self.conv1_3 = BasicConvBlock(channels[1]*3 + channels[2], channels[1])\n\n        # Cột j=4\n        self.conv0_4 = BasicConvBlock(channels[0]*4 + channels[1], channels[0])\n\n        # --- Tích hợp MaxwellSolver vào các node giải mã cuối cùng ---\n        # Chúng ta sẽ giữ lại cơ chế này để bảo toàn tính chất của mô hình\n        self.maxwell_solver1 = MaxwellSolver(channels[0]*2 + channels[1])\n        self.maxwell_solver2 = MaxwellSolver(channels[0]*3 + channels[1])\n        self.maxwell_solver3 = MaxwellSolver(channels[0]*4 + channels[1])\n        # Solver cho decoder path cuối cùng (tương tự U-Net gốc)\n        self.final_decoder_maxwell_solver = MaxwellSolver(channels[0] + channels[1]) \n\n        # --- Lớp đầu ra cho Deep Supervision ---\n        if self.deep_supervision:\n            self.final1 = nn.Conv2d(channels[0], n_classes, kernel_size=1)\n            self.final2 = nn.Conv2d(channels[0], n_classes, kernel_size=1)\n            self.final3 = nn.Conv2d(channels[0], n_classes, kernel_size=1)\n            self.final4 = nn.Conv2d(channels[0], n_classes, kernel_size=1)\n        else:\n            self.final = nn.Conv2d(channels[0], n_classes, kernel_size=1)\n\n    def forward(self, x):\n        # --- Encoder Path ---\n        x0_0 = self.conv0_0(x)\n        x1_0 = self.conv1_0(self.pool(x0_0))\n        x2_0 = self.conv2_0(self.pool(x1_0))\n        x3_0 = self.conv3_0(self.pool(x2_0))\n        x4_0 = self.conv4_0(self.pool(x3_0)) # Bottleneck\n\n        # --- Skip Path & Decoder ---\n        # Cột 1\n        x0_1_input = torch.cat([x0_0, self.up(x1_0)], 1)\n        x0_1 = self.conv0_1(x0_1_input)\n        es_final_decoder = self.final_decoder_maxwell_solver(x0_1_input)\n\n        x1_1 = self.conv1_1(torch.cat([x1_0, self.up(x2_0)], 1))\n        x2_1 = self.conv2_1(torch.cat([x2_0, self.up(x3_0)], 1))\n        x3_1 = self.conv3_1(torch.cat([x3_0, self.up(x4_0)], 1))\n\n        # Cột 2\n        x0_2_input = torch.cat([x0_0, x0_1, self.up(x1_1)], 1)\n        x0_2 = self.conv0_2(x0_2_input)\n        es1 = self.maxwell_solver1(x0_2_input)\n        \n        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.up(x2_1)], 1))\n        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.up(x3_1)], 1))\n\n        # Cột 3\n        x0_3_input = torch.cat([x0_0, x0_1, x0_2, self.up(x1_2)], 1)\n        x0_3 = self.conv0_3(x0_3_input)\n        es2 = self.maxwell_solver2(x0_3_input)\n\n        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.up(x2_2)], 1))\n\n        # Cột 4\n        x0_4_input = torch.cat([x0_0, x0_1, x0_2, x0_3, self.up(x1_3)], 1)\n        x0_4 = self.conv0_4(x0_4_input)\n        es3 = self.maxwell_solver3(x0_4_input)\n\n        # --- Thu thập các kết quả vật lý ---\n        # Ta lấy từ các node giải mã cuối cùng để giữ lại ý nghĩa vật lý\n        all_es_tuples = (es1, es2, es3, es_final_decoder)\n        \n        if self.deep_supervision:\n            output1 = self.final1(x0_1)\n            output2 = self.final2(x0_2)\n            output3 = self.final3(x0_3)\n            output4 = self.final4(x0_4)\n            return [output1, output2, output3, output4], all_es_tuples\n        else:\n            output = self.final(x0_4)\n            return output, all_es_tuples","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T19:55:11.367478Z","iopub.execute_input":"2025-10-07T19:55:11.367782Z","iopub.status.idle":"2025-10-07T19:55:11.387104Z","shell.execute_reply.started":"2025-10-07T19:55:11.367758Z","shell.execute_reply":"2025-10-07T19:55:11.386456Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loss","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import Any, Optional, Dict, List\n\nclass FocalTverskyLoss(nn.Module):\n    \"\"\"\n    Hàm mất mát Focal Tversky Loss.\n    Kết hợp Tversky Index để xử lý mất cân bằng class và Focal Loss để tập trung vào các mẫu khó.\n    \"\"\"\n    def __init__(self, \n                 num_classes: int, \n                 alpha: float = 0.3, \n                 beta: float = 0.7, \n                 gamma: float = 4.0 / 3.0, \n                 epsilon: float = 1e-6):\n        \"\"\"\n        Args:\n            num_classes (int): Số lượng class phân vùng (bao gồm cả background).\n            alpha (float): Trọng số cho False Positives (FP).\n            beta (float): Trọng số cho False Negatives (FN).\n            gamma (float): Tham số focal. Giá trị > 1 để tập trung vào mẫu khó.\n            epsilon (float): Hằng số nhỏ để tránh chia cho 0.\n        \"\"\"\n        super().__init__()\n        self.num_classes = num_classes\n        self.alpha = alpha\n        self.beta = beta\n        self.gamma = gamma\n        self.epsilon = epsilon\n\n    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            logits (torch.Tensor): Đầu ra raw từ model, shape (B, C, H, W).\n            targets (torch.Tensor): Ground truth, shape (B, H, W).\n\n        Returns:\n            torch.Tensor: Giá trị loss vô hướng.\n        \"\"\"\n        # Áp dụng softmax để có xác suất\n        probs = F.softmax(logits, dim=1)\n        \n        # Chuyển target sang dạng one-hot\n        targets_one_hot = F.one_hot(targets.long(), num_classes=self.num_classes).permute(0, 3, 1, 2).float()\n\n        class_losses = []\n        # Bỏ qua background (class 0) vì nó thường chiếm ưu thế và dễ đoán\n        for class_idx in range(1, self.num_classes):\n            pred_class = probs[:, class_idx, :, :]\n            target_class = targets_one_hot[:, class_idx, :, :]\n            \n            # Làm phẳng tensor để tính toán\n            pred_flat = pred_class.contiguous().view(-1)\n            target_flat = target_class.contiguous().view(-1)\n\n            # Tính các thành phần True Positives (TP), False Positives (FP), False Negatives (FN)\n            tp = torch.sum(pred_flat * target_flat)\n            fp = torch.sum(pred_flat * (1 - target_flat))\n            fn = torch.sum((1 - pred_flat) * target_flat)\n            \n            # Tính Tversky Index (TI)\n            tversky_index = (tp + self.epsilon) / (tp + self.alpha * fp + self.beta * fn + self.epsilon)\n            \n            # Tính Focal Tversky Loss (FTL) cho class hiện tại\n            # **Sử dụng công thức đã được sửa đổi và kiểm chứng: (1 - TI)^γ**\n            focal_tversky_loss = torch.pow(1 - tversky_index, self.gamma)\n            \n            class_losses.append(focal_tversky_loss)\n            \n        # Lấy trung bình loss của các class foreground\n        if not class_losses:\n             return torch.tensor(0.0, device=logits.device) # Tránh lỗi nếu chỉ có 1 class\n\n        total_loss = torch.mean(torch.stack(class_losses))\n        \n        return total_loss\n\nclass FocalLoss(nn.Module):\n    \"\"\"\n    Hàm mất mát Focal Loss cho bài toán phân vùng đa lớp.\n    Kế thừa từ https://github.com/Hsuxu/Loss_ToolBox-PyTorch/blob/master/FocalLoss/focal_loss.py\n    \"\"\"\n    def __init__(self,\n                 gamma: float = 2.0,\n                 alpha: Optional[torch.Tensor] = None,\n                 reduction: str = 'mean'):\n        \"\"\"\n        Args:\n            gamma (float): Tham số focal. Giá trị càng lớn, mô hình càng tập trung vào mẫu khó.\n            alpha (torch.Tensor, optional): Trọng số cho mỗi class, shape (C,).\n            reduction (str, optional): 'mean', 'sum' hoặc 'none'.\n        \"\"\"\n        super().__init__()\n        self.gamma = gamma\n        self.alpha = alpha\n        self.reduction = reduction\n\n    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            logits (torch.Tensor): Đầu ra raw từ model, shape (B, C, H, W).\n            targets (torch.Tensor): Ground truth, shape (B, H, W).\n\n        Returns:\n            torch.Tensor: Giá trị loss vô hướng.\n        \"\"\"\n        # Tính CE loss gốc\n        ce_loss = F.cross_entropy(logits, targets.long(), reduction='none')\n        \n        # Lấy xác suất của class đúng (p_t)\n        # pt.shape: (B, H, W)\n        pt = torch.exp(-ce_loss)\n        \n        # Tính Focal Loss\n        # (1-pt)^gamma * ce_loss\n        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n\n        if self.alpha is not None:\n            if self.alpha.device != focal_loss.device:\n                self.alpha = self.alpha.to(focal_loss.device)\n            \n            # Lấy alpha tương ứng với từng pixel\n            alpha_t = self.alpha.gather(0, targets.view(-1)).view_as(targets)\n            focal_loss = alpha_t * focal_loss\n\n        if self.reduction == 'mean':\n            return focal_loss.mean()\n        elif self.reduction == 'sum':\n            return focal_loss.sum()\n        else:\n            return focal_loss\n\n# Lớp PhysicsLoss đã được tái cấu trúc\nclass PhysicsLoss(nn.Module):\n    def __init__(self): # <-- Không cần tham số in_channels_solver nữa\n        super().__init__()\n        # Định nghĩa hằng số vật lý k0 ở đây\n        omega, mu_0, eps_0 = 2 * np.pi * 42.58e6, 4 * np.pi * 1e-7, 8.854187817e-12\n        self.k0 = torch.tensor(omega * np.sqrt(mu_0 * eps_0), dtype=torch.float32)\n\n    def forward(self, b1, eps, sig):\n        # Chuyển các tensor lên đúng device của b1\n        eps = eps.to(b1.device)\n        sig = sig.to(b1.device)\n        \n        # Gọi hàm độc lập\n        residual = compute_helmholtz_residual(b1, eps, sig, self.k0)\n        return torch.mean(residual)\n\n\nclass SmoothnessLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        dy = torch.abs(x[:, :, 1:, :] - x[:, :, :-1, :])\n        dx = torch.abs(x[:, :, :, 1:] - x[:, :, :, :-1])\n        return torch.mean(dy) + torch.mean(dx)\n\n\nclass AnatomicalRuleLoss(nn.Module):\n    \"\"\"\n    Tính toán loss dựa trên quy tắc giải phẫu về vị trí tương đối của các vùng tim.\n    - Phạt khi Tâm thất trái (LV) không được bao quanh bởi Cơ tim (MYO).\n    - Phạt khi Tâm thất phải (RV) nằm cạnh Cơ tim (MYO).\n    \"\"\"\n    def __init__(self, class_indices: Dict[str, int]):\n        \"\"\"\n        Args:\n            class_indices (Dict[str, int]): Dictionary ánh xạ tên class sang chỉ số.\n                                          Cần chứa các key: 'LV', 'MYO', 'RV'.\n        \"\"\"\n        super().__init__()\n        if not all(k in class_indices for k in ['LV', 'MYO', 'RV']):\n            raise ValueError(\"class_indices must contain keys 'LV', 'MYO', and 'RV'.\")\n        self.class_indices = class_indices\n\n    def forward(self, logits: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            logits (torch.Tensor): Đầu ra raw từ model, shape (B, C, H, W).\n\n        Returns:\n            torch.Tensor: Giá trị loss vô hướng.\n        \"\"\"\n        pred_probs = torch.softmax(logits, dim=1)\n        \n        # Lấy bản đồ xác suất cho từng class\n        lv_prob = pred_probs[:, self.class_indices['LV']]\n        myo_prob = pred_probs[:, self.class_indices['MYO']]\n        rv_prob = pred_probs[:, self.class_indices['RV']]\n\n        # Mô phỏng phép giãn nở (dilation) bằng max_pool2d để tìm vùng lân cận\n        dilated_lv_prob = F.max_pool2d(lv_prob.unsqueeze(1), kernel_size=3, stride=1, padding=1).squeeze(1)\n        dilated_rv_prob = F.max_pool2d(rv_prob.unsqueeze(1), kernel_size=3, stride=1, padding=1).squeeze(1)\n\n        # Phạt 1: Vùng bao quanh LV (dilated_lv_prob) không phải là MYO\n        loss1 = dilated_lv_prob * (1 - myo_prob)\n\n        # Phạt 2: Phạt khi vùng bao quanh LV lại là RV\n        loss2 = dilated_lv_prob * rv_prob\n\n        # Kết hợp và lấy trung bình\n        total_rule_loss = torch.mean(loss1 + loss2)\n        return total_rule_loss\n\n\nclass DynamicLossWeighter(nn.Module):\n    \"\"\"\n    Điều chỉnh trọng số cho nhiều thành phần loss một cách tự động,\n    đảm bảo tổng các trọng số luôn bằng 1 bằng cách sử dụng Softmax.\n    \"\"\"\n    def __init__(self, num_losses: int, tau: float = 1.0, initial_weights: Optional[List[float]] = None):\n        \"\"\"\n        Args:\n            num_losses (int): Số lượng thành phần loss cần cân bằng.\n            tau (float): Hệ số nhiệt độ (temperature) cho hàm softmax.\n                         - tau > 1: làm cho các trọng số \"mềm\" hơn (gần bằng nhau hơn).\n                         - 0 < tau < 1: làm cho các trọng số \"cứng\" hơn (chênh lệch nhiều hơn).\n                         - tau = 1: softmax tiêu chuẩn.\n            initial_weights (Optional[List[float]]): Trọng số khởi tạo. Phải có tổng bằng 1.\n                                                     Nếu là None, sẽ khởi tạo đều.\n        \"\"\"\n        super().__init__()\n        assert num_losses > 0, \"Number of losses must be positive\"\n        assert tau > 0, \"Temperature (tau) must be positive\"\n        self.num_losses = num_losses\n        self.tau = tau\n\n        if initial_weights:\n            assert len(initial_weights) == num_losses, \\\n                f\"Number of initial weights ({len(initial_weights)}) must be equal to num_losses ({num_losses})\"\n            initial_weights_tensor = torch.tensor(initial_weights, dtype=torch.float32)\n            assert torch.isclose(initial_weights_tensor.sum(), torch.tensor(1.0)), \\\n                \"Sum of initial weights must be 1\"\n            # Khởi tạo tham số logit từ log của trọng số ban đầu\n            # để softmax(params) xấp xỉ initial_weights\n            initial_params = torch.log(initial_weights_tensor)\n        else:\n            # Khởi tạo bằng 0 sẽ cho ra các trọng số đều nhau sau khi qua softmax\n            initial_params = torch.zeros(num_losses, dtype=torch.float32)\n\n        # 'params' là các logit thô mà optimizer sẽ học\n        self.params = nn.Parameter(initial_params)\n\n    def forward(self, individual_losses: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Tính toán tổng loss đã được cân bằng trọng số.\n\n        Args:\n            individual_losses (torch.Tensor): Một tensor 1D chứa các giá trị loss\n                                              của từng thành phần.\n\n        Returns:\n            torch.Tensor: Giá trị loss tổng hợp (scalar).\n        \"\"\"\n        if not isinstance(individual_losses, torch.Tensor):\n            individual_losses = torch.stack(individual_losses)\n\n        assert individual_losses.dim() == 1 and individual_losses.size(0) == self.num_losses, \\\n            f\"Input individual_losses must be a 1D tensor of size {self.num_losses}\"\n\n        # 1. Tính toán các trọng số bằng cách áp dụng softmax lên các tham số có thể học\n        weights = F.softmax(self.params / self.tau, dim=0)\n\n        # 2. Tính loss tổng hợp bằng cách nhân các loss thành phần với trọng số tương ứng\n        # Đây là phép nhân element-wise và sau đó tính tổng (dot product)\n        total_loss = torch.sum(weights * individual_losses)\n\n        return total_loss\n\n    def get_current_weights(self) -> Dict[str, float]:\n        \"\"\"\n        Lấy các giá trị trọng số hiện tại để theo dõi.\n        Các trọng số này có tổng bằng 1.\n        \"\"\"\n        with torch.no_grad():\n            weights = F.softmax(self.params / self.tau, dim=0)\n            return {f\"weight_{i}\": w.item() for i, w in enumerate(weights)}\n\n\nclass CombinedLoss(nn.Module):\n    \"\"\"\n    Combined loss được cập nhật để sử dụng Focal Loss thay cho CE Loss.\n    \"\"\"\n    def __init__(self, \n                 num_classes=4, \n                 initial_loss_weights: Optional[List[float]] = None,\n                 class_indices_for_rules: Dict[str, int] = None):\n        super().__init__()\n        \n        # --- XÓA BỎ ClassWeightUpdater ---\n        # self.class_weighter = ClassWeightUpdater(num_classes=num_classes)\n        \n        # --- Initialize loss components ---\n        \n        # 1. THAY THẾ Cross Entropy BẰNG FOCAL LOSS\n        self.fl = FocalLoss(gamma=2.0)\n        print(\"Initialized with Focal Loss (gamma=2.0).\")\n        \n        # 2. FOCAL TVERSKY LOSS (giữ nguyên)\n        self.ftl = FocalTverskyLoss(\n            num_classes=num_classes, \n            alpha=0.2, \n            beta=0.8, \n            gamma=4.0/3.0\n        )\n        print(\"Initialized with Focal Tversky Loss (alpha=0.3, beta=0.7, gamma=4/3).\")\n\n        # 3. Physics Loss (giữ nguyên)\n        self.pl = PhysicsLoss()\n        \n        # 4. Anatomical Rule Loss (giữ nguyên)\n        if class_indices_for_rules is None:\n            raise ValueError(\"`class_indices_for_rules` must be provided.\")\n        self.arl = AnatomicalRuleLoss(class_indices=class_indices_for_rules)\n        \n        # Khởi tạo bộ cân bằng trọng số cho 4 thành phần\n        self.loss_weighter = DynamicLossWeighter(num_losses=4, initial_weights=initial_loss_weights)\n\n    def forward(self, logits, targets, b1=None, all_es=None):\n        # --- XÓA BỎ Step 1: Không cần cập nhật trọng số class động nữa ---\n        \n        # --- Step 2: Calculate individual loss components ---\n        l_fl = self.fl(logits, targets) # Tính Focal Loss\n        l_ftl = self.ftl(logits, targets) # Tính Focal Tversky Loss\n\n        lphy = torch.tensor(0.0, device=logits.device)\n        if self.pl is not None and b1 is not None and all_es:\n            # ... (phần tính lphy giữ nguyên)\n            try:\n                e1, s1 = all_es[0]\n                lphy = self.pl(b1, e1, s1)\n            except (IndexError, TypeError):\n                print(\"Warning: Physics loss skipped due to unexpected `all_es` format.\")\n        \n        larl = self.arl(logits)\n\n        # --- Step 3: Kết hợp 4 thành phần loss ---\n        individual_losses = torch.stack([l_fl, l_ftl, lphy, larl])\n        total_loss = self.loss_weighter(individual_losses)\n\n        return total_loss\n\n    def get_current_loss_weights(self) -> Dict[str, float]:\n        \"\"\"Helper để theo dõi trọng số giữa các hàm loss.\"\"\"\n        weights = self.loss_weighter.get_current_weights()\n        # Cập nhật tên cho rõ ràng\n        return {\n            \"weight_FocalLoss\": weights[\"weight_0\"],\n            \"weight_FocalTverskyLoss\": weights[\"weight_1\"],\n            \"weight_Physics\": weights[\"weight_2\"],\n            \"weight_Anatomical\": weights[\"weight_3\"]\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T19:55:11.387805Z","iopub.execute_input":"2025-10-07T19:55:11.388003Z","iopub.status.idle":"2025-10-07T19:55:11.416429Z","shell.execute_reply.started":"2025-10-07T19:55:11.387980Z","shell.execute_reply":"2025-10-07T19:55:11.415747Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading Data","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nimport numpy as np\n\n# Giả định các hàm adaptive_quantum_noise_injection và ePURE đã được định nghĩa ở đâu đó\n# import {ePURE, adaptive_quantum_noise_injection} from '...'\n\nclass ACDCDataset25D(Dataset):\n    \"\"\"\n    Dataset cho ACDC, nạp dữ liệu 2.5D.\n    NÂNG CẤP:\n    - Tùy chỉnh số lát cắt đầu vào.\n    - Tích hợp thêm nhiễu lượng tử thích nghi như một bước augmentation.\n    \"\"\"\n    def __init__(self, volumes_list, masks_list, num_input_slices=5, transforms=None, \n                 noise_injector_model=None, device='cpu'): # <-- THAY ĐỔI: Thêm 2 tham số mới\n        \"\"\"\n        Args:\n            volumes_list (list): Danh sách các volume ảnh 3D.\n            masks_list (list): Danh sách các volume mask 3D tương ứng.\n            num_input_slices (int): Số lát cắt liên tục để xếp chồng.\n            transforms (albumentations.Compose): Pipeline các phép biến đổi hình học.\n            noise_injector_model (nn.Module, optional): Mô hình ePURE để tạo noise map.\n            device (str): Thiết bị để chạy noise_injector_model.\n        \"\"\"\n        if num_input_slices % 2 == 0:\n            raise ValueError(\"num_input_slices phải là một số lẻ.\")\n            \n        self.volumes = volumes_list\n        self.masks = masks_list\n        self.num_input_slices = num_input_slices\n        self.transforms = transforms\n        self.noise_injector_model = noise_injector_model # <-- THÊM MỚI\n        self.device = device # <-- THÊM MỚI\n        \n        self.index_map = []\n        for vol_idx, vol in enumerate(self.volumes):\n            radius = (self.num_input_slices - 1) // 2\n            num_slices = vol.shape[2]\n            for slice_idx in range(radius, num_slices - radius):\n                self.index_map.append((vol_idx, slice_idx))\n    \n    def __len__(self):\n        return len(self.index_map)\n\n    # Bên trong lớp ACDCDataset25D\n\n    def __getitem__(self, idx):\n        vol_idx, center_slice_idx = self.index_map[idx]\n        \n        current_volume = self.volumes[vol_idx]\n        current_mask_volume = self.masks[vol_idx]\n        num_slices_in_vol = current_volume.shape[2]\n    \n        radius = (self.num_input_slices - 1) // 2\n        offsets = range(-radius, radius + 1)\n        \n        slice_indices = [np.clip(center_slice_idx + offset, 0, num_slices_in_vol - 1) for offset in offsets]\n        \n        image_stack = np.stack(\n            [current_volume[:, :, i] for i in slice_indices],\n            axis=-1\n        ).astype(np.float32)\n        \n        mask = current_mask_volume[:, :, center_slice_idx]\n        \n        if self.transforms:\n            augmented = self.transforms(image=image_stack, mask=mask)\n            image_tensor = augmented['image']\n            mask_tensor = augmented['mask']\n        else:\n            image_tensor = torch.from_numpy(image_stack).permute(2, 0, 1)\n            mask_tensor = torch.from_numpy(mask)\n    \n        # --- SỬA LỖI LOGIC TĂNG CƯỜNG DỮ LIỆU ---\n        if self.noise_injector_model is not None:\n            with torch.no_grad():\n                # Chuyển ảnh lên device để tạo noise map.\n                # Tensor này đã có chiều batch và ở trên GPU.\n                img_on_gpu_with_batch = image_tensor.to(self.device).unsqueeze(0)\n                noise_map = self.noise_injector_model(img_on_gpu_with_batch)\n                \n                # Áp dụng nhiễu lượng tử.\n                # Cả hai đầu vào bây giờ đều ở trên GPU, nên sẽ không có lỗi.\n                image_tensor_with_noise_gpu = adaptive_quantum_noise_injection(\n                    img_on_gpu_with_batch, # <-- SỬA ĐỔI: Dùng tensor đã ở trên GPU\n                    noise_map\n                )\n                \n                # Chuyển kết quả cuối cùng về lại CPU và bỏ chiều batch\n                image_tensor = image_tensor_with_noise_gpu.squeeze(0).cpu() # <-- SỬA ĐỔI\n                \n        return image_tensor, mask_tensor.long()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T19:55:11.417082Z","iopub.execute_input":"2025-10-07T19:55:11.417299Z","iopub.status.idle":"2025-10-07T19:55:11.431780Z","shell.execute_reply.started":"2025-10-07T19:55:11.417285Z","shell.execute_reply":"2025-10-07T19:55:11.431236Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport nibabel as nib\nimport numpy as np\nfrom skimage.transform import resize\nimport sys\nimport configparser\n\ndef load_acdc_volumes(directory, target_size=(224, 224), max_patients=None):\n    volumes_list = []\n    masks_list = []\n    \n    if not os.path.exists(directory):\n        print(f\"Lỗi: Không tìm thấy thư mục dataset tại {directory}.\", file=sys.stderr)\n        return [], []\n\n    patient_folders = sorted([d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))])\n    patient_count = 0\n\n    for patient_folder in patient_folders:\n        if max_patients and patient_count >= max_patients:\n            break\n\n        patient_path = os.path.join(directory, patient_folder)\n        info_cfg_path = os.path.join(patient_path, 'Info.cfg')\n\n        # --- Đọc frame ED/ES từ file Info.cfg ---\n        ed_frame, es_frame = -1, -1\n        if os.path.exists(info_cfg_path):\n            parser = configparser.ConfigParser()\n            try:\n                with open(info_cfg_path, 'r') as f:\n                    config_string = '[DEFAULT]\\n' + f.read()\n                parser.read_string(config_string)\n                ed_frame = int(parser['DEFAULT']['ED'])\n                es_frame = int(parser['DEFAULT']['ES'])\n            except Exception as e:\n                print(f\"Cảnh báo: Không thể đọc Info.cfg cho {patient_folder}: {e}. Bỏ qua bệnh nhân.\", file=sys.stderr)\n                continue\n        else:\n            print(f\"Cảnh báo: Không tìm thấy Info.cfg cho {patient_folder}. Bỏ qua bệnh nhân.\", file=sys.stderr)\n            continue\n            \n        ed_img_filename = f'{patient_folder}_frame{ed_frame:02d}.nii'\n        es_img_filename = f'{patient_folder}_frame{es_frame:02d}.nii'\n        ed_mask_filename = f'{patient_folder}_frame{ed_frame:02d}_gt.nii'\n        es_mask_filename = f'{patient_folder}_frame{es_frame:02d}_gt.nii'\n\n        ed_img_path = os.path.join(patient_path, ed_img_filename)\n        es_img_path = os.path.join(patient_path, es_img_filename)\n        ed_mask_path = os.path.join(patient_path, ed_mask_filename)\n        es_mask_path = os.path.join(patient_path, es_mask_filename)\n\n        # --- Hàm helper để nạp và xử lý một volume 3D (giữ nguyên) ---\n        def _load_nifti_volume(img_fpath, mask_fpath, target_sz):\n            try:\n                if not os.path.exists(img_fpath):\n                    # Dòng print này sẽ không xuất hiện nữa sau khi sửa tên file\n                    # print(f\"DEBUG: File not found at {img_fpath}\") \n                    return None, None\n\n                img_nifti = nib.load(img_fpath)\n                img_data = img_nifti.get_fdata()\n\n                mask_data = None\n                if os.path.exists(mask_fpath):\n                    mask_nifti = nib.load(mask_fpath)\n                    mask_data = mask_nifti.get_fdata()\n\n                num_slices = img_data.shape[2]\n                resized_img_vol = np.zeros((target_sz[0], target_sz[1], num_slices), dtype=np.float32)\n                \n                resized_mask_vol = None\n                if mask_data is not None:\n                    resized_mask_vol = np.zeros((target_sz[0], target_sz[1], num_slices), dtype=np.uint8)\n\n                for i in range(num_slices):\n                    resized_img_vol[:, :, i] = resize(\n                        img_data[:, :, i], target_sz, order=1, preserve_range=True,\n                        anti_aliasing=True, mode='reflect'\n                    )\n                    if mask_data is not None:\n                        resized_mask_vol[:, :, i] = resize(\n                            mask_data[:, :, i], target_sz, order=0, preserve_range=True,\n                            anti_aliasing=False, mode='reflect'\n                        )\n                \n                return resized_img_vol, resized_mask_vol\n            except Exception as e:\n                print(f\"Lỗi khi xử lý volume {img_fpath}: {e}\", file=sys.stderr)\n                return None, None\n\n        # --- Nạp và thêm các volume vào danh sách ---\n        ed_vol, ed_mask_vol = _load_nifti_volume(ed_img_path, ed_mask_path, target_size)\n        if ed_vol is not None:\n            volumes_list.append(ed_vol)\n            if ed_mask_vol is not None:\n                masks_list.append(ed_mask_vol)\n\n        es_vol, es_mask_vol = _load_nifti_volume(es_img_path, es_mask_path, target_size)\n        if es_vol is not None:\n            volumes_list.append(es_vol)\n            if es_mask_vol is not None:\n                masks_list.append(es_mask_vol)\n        \n        patient_count += 1\n        \n    return volumes_list, masks_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T19:55:11.432681Z","iopub.execute_input":"2025-10-07T19:55:11.433167Z","iopub.status.idle":"2025-10-07T19:55:12.410436Z","shell.execute_reply.started":"2025-10-07T19:55:11.433150Z","shell.execute_reply":"2025-10-07T19:55:12.409657Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Metrics","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport numpy as np\n\ndef evaluate_metrics(model, dataloader, device, num_classes=4):\n    \"\"\"\n    Hàm đánh giá các chỉ số cho mô hình phân đoạn.\n    Đã được cập nhật để tương thích với output dạng list từ UNet++ (deep supervision).\n    \"\"\"\n    model.eval()\n    tp = [0] * num_classes\n    fp = [0] * num_classes\n    fn = [0] * num_classes\n    dice_s = [0.0] * num_classes\n    iou_s = [0.0] * num_classes\n    batches = 0\n\n    total_correct_pixels = 0\n    total_pixels = 0\n\n    with torch.no_grad():\n        for imgs, tgts in dataloader:\n            imgs, tgts = imgs.to(device), tgts.to(device)\n            if imgs.size(0) == 0: continue\n            \n            # --- SỬA ĐỔI CHÍNH Ở ĐÂY ---\n            # Model UNet++ trả về một list các logits.\n            # Khi đánh giá, chúng ta chỉ quan tâm đến output cuối cùng, chi tiết nhất.\n            logits_list, _ = model(imgs)\n            \n            # Lấy ra output cuối cùng từ danh sách (đây là dự đoán tốt nhất)\n            logits = logits_list[-1] \n            # --- KẾT THÚC SỬA ĐỔI ---\n            \n            preds = torch.argmax(F.softmax(logits, dim=1), dim=1)\n            batches += 1\n            total_correct_pixels += (preds == tgts).sum().item()\n            total_pixels += tgts.numel()\n\n            for c in range(num_classes):\n                pc_f = (preds == c).float().view(-1)\n                tc_f = (tgts == c).float().view(-1)\n                inter = (pc_f * tc_f).sum()\n\n                dice_s[c] += ((2. * inter + 1e-6) / (pc_f.sum() + tc_f.sum() + 1e-6)).item()\n                iou_s[c] += ((inter + 1e-6) / (pc_f.sum() + tc_f.sum() - inter + 1e-6)).item()\n                tp[c] += inter.item()\n                fp[c] += (pc_f.sum() - inter).item()\n                fn[c] += (tc_f.sum() - inter).item()\n\n    metrics = {'accuracy': 0.0, 'dice_scores': [], 'iou': [], 'precision': [], 'recall': [], 'f1_score': []}\n\n    if batches > 0:\n        if total_pixels > 0:\n            metrics['accuracy'] = total_correct_pixels / total_pixels\n        \n        for c in range(num_classes):\n            metrics['dice_scores'].append(dice_s[c] / batches)\n            metrics['iou'].append(iou_s[c] / batches)\n            prec = tp[c] / (tp[c] + fp[c] + 1e-6)\n            rec = tp[c] / (tp[c] + fn[c] + 1e-6)\n            metrics['precision'].append(prec)\n            metrics['recall'].append(rec)\n            metrics['f1_score'].append(2 * prec * rec / (prec + rec + 1e-6) if (prec + rec > 0) else 0.0)\n    else:\n        for _ in range(num_classes):\n            [metrics[key].append(0.0) for key in ['dice_scores', 'iou', 'precision', 'recall', 'f1_score']]\n            \n    return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T19:55:12.411274Z","iopub.execute_input":"2025-10-07T19:55:12.411645Z","iopub.status.idle":"2025-10-07T19:55:12.421510Z","shell.execute_reply.started":"2025-10-07T19:55:12.411618Z","shell.execute_reply":"2025-10-07T19:55:12.420833Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# --- HÀM IN THAM SỐ ---\n# =============================================================================\n\ndef print_model_parameters(model):\n    \"\"\"\n    Hàm này sẽ in ra số lượng tham số của từng khối con trong mô hình\n    và tổng số tham số cuối cùng.\n    \"\"\"\n    print(\"=\"*60)\n    print(\"PHÂN TÍCH THAM SỐ MÔ HÌNH RobustMedVFL_UNet\")\n    print(\"=\"*60)\n\n    total_params = 0\n    \n    # Duyệt qua từng attribute (khối con) của mô hình\n    for name, module in model.named_children():\n        # Chỉ tính các khối có tham số (bỏ qua MaxPool, Upsample,...)\n        if list(module.parameters()):\n            params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n            print(f\"- {name:<30}: {params:>12,}\")\n            total_params += params\n\n    print(\"=\"*60)\n    print(f\"TỔNG CỘNG                      : {total_params:>12,}\")\n    print(\"=\"*60)\n    \n    # Xác minh lại bằng cách tính tổng trực tiếp từ model.parameters()\n    direct_total = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"Xác minh (tổng trực tiếp)       : {direct_total:>12,}\")\n    print(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T19:55:12.422276Z","iopub.execute_input":"2025-10-07T19:55:12.422629Z","iopub.status.idle":"2025-10-07T19:55:12.443308Z","shell.execute_reply.started":"2025-10-07T19:55:12.422608Z","shell.execute_reply":"2025-10-07T19:55:12.442657Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport numpy as np\nimport time\nimport os\nimport h5py\nfrom skimage.transform import resize\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom itertools import chain\nimport cv2 \nimport torch.multiprocessing as mp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T19:55:12.444117Z","iopub.execute_input":"2025-10-07T19:55:12.444348Z","iopub.status.idle":"2025-10-07T19:55:14.779044Z","shell.execute_reply.started":"2025-10-07T19:55:12.444328Z","shell.execute_reply":"2025-10-07T19:55:14.778104Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Configuration ---\nNUM_EPOCHS_CENTRALIZED = 250\nNUM_CLASSES = 4\nLEARNING_RATE = 1e-3\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nIMG_SIZE = 224\nBATCH_SIZE = 24\nNUM_SLICES = 5 # <-- THÊM MỚI: Định nghĩa số lát cắt ở một nơi\nEARLY_STOP_PATIENCE = 30 # <-- THÊM MỚI: Định nghĩa patience cho Early Stopping","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T19:55:14.779956Z","iopub.execute_input":"2025-10-07T19:55:14.780671Z","iopub.status.idle":"2025-10-07T19:55:14.863251Z","shell.execute_reply.started":"2025-10-07T19:55:14.780650Z","shell.execute_reply":"2025-10-07T19:55:14.862435Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    mp.set_start_method('spawn', force=True)\n\n    print(f\"Thiết bị đang sử dụng: {DEVICE}\")\n    \n    # --- Augmentation pipelines ---\n    train_transform = A.Compose([\n        A.Rotate(limit=20, p=0.7),\n        A.HorizontalFlip(p=0.5),\n        A.ElasticTransform(p=0.5, alpha=120, sigma=120 * 0.05),\n        A.Affine(\n            scale=(0.9, 1.1),\n            translate_percent=(-0.0625, 0.0625),\n            rotate=(-15, 15),\n            p=0.7,\n            border_mode=cv2.BORDER_CONSTANT\n        ),\n        A.RandomBrightnessContrast(p=0.5),\n        ToTensorV2(),\n    ])\n    val_test_transform = A.Compose([\n        ToTensorV2(),\n    ])\n\n    # --- Part 1: Nạp và Chuẩn bị Dữ liệu ---\n    base_dataset_root = '/kaggle/input/automated-cardiac-diagnosis-challenge-miccai17/database'\n    train_data_path = os.path.join(base_dataset_root, 'training')\n    test_data_path = os.path.join(base_dataset_root, 'testing')\n    \n    print(f\"Nạp các volume training từ: {train_data_path}...\")\n    all_train_volumes, all_train_masks = load_acdc_volumes(\n        train_data_path, target_size=(IMG_SIZE, IMG_SIZE)\n    )\n    print(f\"Đã nạp {len(all_train_volumes)} training volumes.\")\n\n    print(f\"Nạp các volume testing từ: {test_data_path}...\")\n    all_test_volumes, all_test_masks = load_acdc_volumes(\n        test_data_path, target_size=(IMG_SIZE, IMG_SIZE)\n    )\n    print(f\"Đã nạp {len(all_test_volumes)} testing volumes.\")\n\n    # Chuẩn hóa cường độ pixel\n    for i in range(len(all_train_volumes)):\n        max_val = np.max(all_train_volumes[i])\n        if max_val > 0: all_train_volumes[i] /= max_val\n    for i in range(len(all_test_volumes)):\n        max_val = np.max(all_test_volumes[i])\n        if max_val > 0: all_test_volumes[i] /= max_val\n\n    # Chia dữ liệu theo volume (bệnh nhân)\n    indices = list(range(len(all_train_volumes)))\n    train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=42)\n\n    X_train_vols = [all_train_volumes[i] for i in train_indices]\n    y_train_vols = [all_train_masks[i] for i in train_indices]\n    X_val_vols = [all_train_volumes[i] for i in val_indices]\n    y_val_vols = [all_train_masks[i] for i in val_indices]\n    \n    # Khởi tạo mô hình ePURE riêng cho việc tăng cường dữ liệu\n    ePURE_augmenter = ePURE(in_channels=NUM_SLICES).to(DEVICE)\n    ePURE_augmenter.eval()\n\n    # --- Tạo Dataset và DataLoader ---\n    train_dataset = ACDCDataset25D(\n        volumes_list=X_train_vols, \n        masks_list=y_train_vols, \n        num_input_slices=NUM_SLICES, \n        transforms=train_transform,\n        noise_injector_model=ePURE_augmenter,\n        device=DEVICE\n    )\n    val_dataset = ACDCDataset25D(\n        volumes_list=X_val_vols, \n        masks_list=y_val_vols, \n        num_input_slices=NUM_SLICES, \n        transforms=val_test_transform\n    )\n    test_dataset = ACDCDataset25D(\n        volumes_list=all_test_volumes, \n        masks_list=all_test_masks, \n        num_input_slices=NUM_SLICES, \n        transforms=val_test_transform\n    )\n\n    train_dataloader = DataLoader(train_dataset, \n                                  batch_size=BATCH_SIZE, \n                                  shuffle=True, \n                                  num_workers=0, \n                                  pin_memory=True)\n    val_dataloader = DataLoader(val_dataset, \n                                batch_size=BATCH_SIZE, \n                                shuffle=False, \n                                num_workers=0, \n                                pin_memory=True)\n    test_dataloader = DataLoader(test_dataset, \n                                 batch_size=BATCH_SIZE, \n                                 shuffle=False, \n                                 num_workers=0, \n                                 pin_memory=True)\n    \n    print(f\"\\nSố mẫu training (lát cắt): {len(train_dataset)}, Validation: {len(val_dataset)}, Test: {len(test_dataset)}\")\n    print(\"-\" * 60)\n\n    # Chuẩn bị tensor cho B1 map calculator\n    def convert_volumes_to_tensor(volumes_list):\n        all_slices = []\n        for vol in volumes_list:\n            for i in range(vol.shape[2]):\n                all_slices.append(torch.from_numpy(vol[:, :, i]).unsqueeze(0))\n        return torch.stack(all_slices, dim=0).float()\n    \n    X_train_tensor_for_b1 = convert_volumes_to_tensor(X_train_vols)\n    X_val_tensor_for_b1 = convert_volumes_to_tensor(X_val_vols)\n    X_test_tensor_for_b1 = convert_volumes_to_tensor(all_test_volumes)\n    \n    # b1_calculator = integrate_b1_map_into_training(\n    #     X_train_tensor_for_b1, X_val_tensor_for_b1, X_test_tensor_for_b1,\n    #     img_size=IMG_SIZE, device=DEVICE\n    # )\n\n\n    # --- SỬA ĐỔI 1: THAY THẾ TOÀN BỘ KHỐI TÍNH B1 MAP CŨ ---\n\n    # Hàm helper vẫn hữu ích để gộp dữ liệu\n    def convert_volumes_to_tensor(volumes_list):\n        all_slices = []\n        for vol in volumes_list:\n            # Chuyển (H, W, Slices) -> (Slices, 1, H, W)\n            for i in range(vol.shape[2]):\n                all_slices.append(torch.from_numpy(vol[:, :, i]).unsqueeze(0))\n        return torch.stack(all_slices, dim=0).float()\n    \n    # Gộp tất cả ảnh từ các tập train, val, test để tính B1 map chung\n    all_images_tensor = convert_volumes_to_tensor(X_train_vols + X_val_vols + all_test_volumes)\n    \n    # Đặt tên file save_path phù hợp với dataset của bạn\n    dataset_name = \"acdc_cardiac\" \n    common_b1_map = calculate_ultimate_common_b1_map(\n        all_images=all_images_tensor,\n        device=DEVICE,\n        save_path=f\"{dataset_name}_ultimate_common_b1_map.pth\"\n    )\n    # --- KẾT THÚC SỬA ĐỔI 1 ---\n    \n    # --- Part 2: Khởi tạo Model, Loss, Optimizer ---\n    print(\"Khởi tạo các thành phần mô hình...\")\n    \n    model = RobustMedVFL_UNet(n_channels=NUM_SLICES, n_classes=NUM_CLASSES).to(DEVICE)\n    print_model_parameters(model)\n    my_class_indices = {'RV': 1, 'MYO': 2, 'LV': 3}\n    criterion = CombinedLoss(\n        num_classes=NUM_CLASSES,\n        initial_loss_weights=[0.4, 0.4, 0.1, 0.1],\n        class_indices_for_rules=my_class_indices\n    ).to(DEVICE)\n    \n    optimizer = torch.optim.AdamW(chain(model.parameters(), criterion.parameters()), lr=LEARNING_RATE)\n    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.2, patience=5, verbose=True)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10)\n    print(\"Tất cả thành phần đã được khởi tạo.\")\n    print(\"-\" * 60)\n    \n    # --- Part 3: Vòng lặp Huấn luyện ---\n    best_val_metric = 0.0\n    epochs_no_improve = 0\n\n    for epoch in range(NUM_EPOCHS_CENTRALIZED):\n        print(f\"\\n--- Epoch {epoch + 1}/{NUM_EPOCHS_CENTRALIZED} ---\")\n        \n        # --- Training phase ---\n        model.train()\n        epoch_train_loss = 0.0\n        for images, targets in train_dataloader:\n            images, targets = images.to(DEVICE), targets.to(DEVICE)\n            \n            optimizer.zero_grad()\n            # b1_map = get_b1_map_for_training(images, b1_calculator)\n            b1_map_for_loss = common_b1_map.expand(images.size(0), -1, -1, -1)\n            logits_list, all_eps_sigma_tuples = model(images)\n\n            total_loss = 0\n            for logits in logits_list:\n                if logits.shape[2:] != targets.shape[1:]:\n                    resized_targets = F.interpolate(\n                        targets.unsqueeze(1).float(), \n                        size=logits.shape[2:], \n                        mode='nearest'\n                    ).squeeze(1).long()\n                else:\n                    resized_targets = targets\n                \n                # loss_component = criterion(logits, resized_targets, b1_map, all_eps_sigma_tuples)\n                loss_component = criterion(logits, resized_targets, b1_map_for_loss, all_eps_sigma_tuples)\n                total_loss += loss_component\n            \n            loss = total_loss / len(logits_list)\n            \n            loss.backward()\n            optimizer.step()\n            \n            epoch_train_loss += loss.item()\n            \n        avg_train_loss = epoch_train_loss / len(train_dataloader)\n        print(f\"   Epoch {epoch+1} - Training Loss: {avg_train_loss:.4f}\")\n\n        # --- Validation phase ---\n        if val_dataloader.dataset and len(val_dataloader.dataset) > 0:\n            print(\"   Evaluating on validation set...\")\n            val_metrics = evaluate_metrics(model, val_dataloader, DEVICE, NUM_CLASSES)\n            \n            # --- 1. KHAI BÁO VÀ TÍNH TOÁN TẤT CẢ CÁC CHỈ SỐ TRƯỚC ---\n            \n            # Lấy các chỉ số từ dictionary trả về\n            val_accuracy = val_metrics['accuracy']\n            all_dice = val_metrics['dice_scores']\n            all_iou = val_metrics['iou']\n            all_precision = val_metrics['precision']\n            all_recall = val_metrics['recall']\n            all_f1 = val_metrics['f1_score']\n            \n            # Tính trung bình trên các lớp foreground (1, 2, 3)\n            avg_fg_dice = np.mean(all_dice[1:])\n            avg_fg_iou = np.mean(all_iou[1:])\n            avg_fg_precision = np.mean(all_precision[1:])\n            avg_fg_recall = np.mean(all_recall[1:])\n            avg_fg_f1 = np.mean(all_f1[1:])\n            \n            # Lấy learning rate hiện tại\n            current_lr = optimizer.param_groups[0]['lr']\n\n            # --- 2. IN ẤN KẾT QUẢ MỘT CÁCH CÓ TỔ CHỨC ---\n            \n            print(\"   --- Per-Class Metrics ---\")\n            class_map = {0: 'BG', 1: 'RV', 2: 'MYO', 3: 'LV'}\n            for c_idx in range(NUM_CLASSES):\n                class_name = class_map.get(c_idx, f\"Class {c_idx}\")\n                print(f\"=> {class_name:<15}: Dice: {all_dice[c_idx]:.4f}, IoU: {all_iou[c_idx]:.4f}, Precision: {all_precision[c_idx]:.4f}, Recall: {all_recall[c_idx]:.4f}, F1: {all_f1[c_idx]:.4f}\")\n\n            print(\"   --- Summary Metrics ---\")\n            print(f\"=> Avg Foreground: Dice: {avg_fg_dice:.4f}, IoU: {avg_fg_iou:.4f}, Precision: {avg_fg_precision:.4f}, Recall: {avg_fg_recall:.4f}, F1: {avg_fg_f1:.4f}\")\n            print(f\"=> Overall Accuracy: {val_accuracy:.4f} | Current Learning Rate: {current_lr:.6f}\")\n\n            # --- 3. CẬP NHẬT SCHEDULER VÀ LƯU MODEL ---\n            \n            scheduler.step(avg_fg_dice)\n            if avg_fg_dice > best_val_metric:\n                best_val_metric = avg_fg_dice\n                torch.save(model.state_dict(), \"best_model.pth\")\n                print(f\"   >>> New best model saved with Avg Foreground Dice: {best_val_metric:.4f} <<<\")\n                epochs_no_improve = 0\n            else:\n                epochs_no_improve += 1\n        else:\n            print(\"   Validation dataset is empty. Skipping validation.\")\n\n        # Kiểm tra điều kiện Early Stopping\n        if epochs_no_improve >= EARLY_STOP_PATIENCE:\n            print(f\"\\nEarly stopping triggered after {EARLY_STOP_PATIENCE} epochs with no improvement.\")\n            break\n\n    print(\"\\n--- Centralized Training Finished ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T19:55:14.864409Z","iopub.execute_input":"2025-10-07T19:55:14.864922Z","iopub.status.idle":"2025-10-07T21:20:00.727835Z","shell.execute_reply.started":"2025-10-07T19:55:14.864893Z","shell.execute_reply":"2025-10-07T21:20:00.727017Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Trực quan kết quả trên tập test set\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport matplotlib.colors as mcolors\nimport os\n\n# --- Các định nghĩa cho việc trực quan hóa (giữ nguyên) ---\nACDC_CLASS_MAP = {\n    0: \"Background\",\n    1: \"Right Ventricle (RV)\",\n    2: \"Myocardium (MYO)\",\n    3: \"Left Ventricle (LV)\"\n}\nACDC_COLOR_MAP = {\n    0: 'black',\n    1: '#FF0000',\n    2: '#00FF00',\n    3: '#0000FF'\n}\n\n# --- THÊM MỚI: Định nghĩa hằng số để dễ quản lý ---\nNUM_SLICES = 5\n\ndef evaluate_metrics_with_tta(model, dataloader, device, num_classes=4):\n    \"\"\"\n    Hàm đánh giá CÓ TÍCH HỢP TTA NÂNG CAO (4 phép biến đổi: gốc, lật ngang, lật dọc, lật cả hai).\n    \"\"\"\n    model.eval()\n    \n    # Khởi tạo các biến để lưu tổng các chỉ số\n    total_dice = np.zeros(num_classes)\n    total_iou = np.zeros(num_classes)\n    total_precision = np.zeros(num_classes)\n    total_recall = np.zeros(num_classes)\n    total_f1 = np.zeros(num_classes)\n    total_correct_pixels = 0\n    total_pixels = 0\n    num_batches = 0\n\n    with torch.no_grad():\n        for imgs, tgts in dataloader:\n            imgs, tgts = imgs.to(device), tgts.to(device)\n            if imgs.size(0) == 0: continue\n            \n            num_batches += 1\n            \n            # --- ADVANCED TTA LOGIC ---\n            # 1. Tạo 4 phiên bản biến đổi\n            img_orig = imgs\n            img_hflip = torch.flip(imgs, dims=[-1])  # Lật ngang\n            # img_vflip = torch.flip(imgs, dims=[-2])  # Lật dọc\n            # img_hvflip = torch.flip(imgs, dims=[-1, -2]) # Lật cả hai chiều\n\n            # Gộp lại thành một batch lớn để dự đoán một lần\n            # tta_batch = torch.cat([img_orig, img_hflip, img_vflip, img_hvflip], dim=0)\n            tta_batch = torch.cat([img_orig, img_hflip], dim=0)\n            # tta_batch = torch.cat([img_orig, img_hflip, img_vflip], dim=0)\n            # 2. Dự đoán trên cả batch\n            logits_list, _ = model(tta_batch)\n            probs_batch = torch.softmax(logits_list[-1], dim=1)\n            \n            # Tách kết quả cho từng phiên bản\n            # prob_orig, prob_hflip, prob_vflip, prob_hvflip = torch.chunk(probs_batch, 4, dim=0)\n            prob_orig, prob_hflip = torch.chunk(probs_batch, 2, dim=0)\n            # prob_orig, prob_hflip, prob_vflip = torch.chunk(probs_batch, 3, dim=0)\n\n            # 3. Hoàn tác các phép biến đổi trên kết quả\n            prob_hflip_restored = torch.flip(prob_hflip, dims=[-1])\n            # prob_vflip_restored = torch.flip(prob_vflip, dims=[-2])\n            # prob_hvflip_restored = torch.flip(prob_hvflip, dims=[-1, -2])\n            \n            # 4. Lấy trung bình 4 bản đồ xác suất\n            # avg_probs = (prob_orig + prob_hflip_restored + prob_vflip_restored + prob_hvflip_restored) / 4.0\n            avg_probs = (prob_orig + prob_hflip_restored) / 2.0\n            \n            # 5. Lấy dự đoán cuối cùng\n            preds = torch.argmax(avg_probs, dim=1)\n            # --- END TTA LOGIC ---\n            \n            # Phần tính toán metrics cho batch hiện tại (giữ nguyên)\n            total_correct_pixels += (preds == tgts).sum().item()\n            total_pixels += tgts.numel()\n\n            for c in range(num_classes):\n                pred_mask = (preds == c)\n                true_mask = (tgts == c)\n                \n                tp = (pred_mask & true_mask).sum().item()\n                fp = (pred_mask & ~true_mask).sum().item()\n                fn = (~pred_mask & true_mask).sum().item()\n                \n                # Tính toán một lần và cộng dồn\n                total_dice[c] += (2. * tp) / (2 * tp + fp + fn + 1e-8)\n                total_iou[c] += tp / (tp + fp + fn + 1e-8)\n                \n                precision = tp / (tp + fp + 1e-8)\n                recall = tp / (tp + fn + 1e-8)\n                \n                total_precision[c] += precision\n                total_recall[c] += recall\n                total_f1[c] += (2 * precision * recall) / (precision + recall + 1e-8)\n\n    # Tính trung bình các metrics trên toàn bộ dataloader (giữ nguyên)\n    metrics = {\n        'accuracy': total_correct_pixels / total_pixels if total_pixels > 0 else 0,\n        'dice_scores': (total_dice / num_batches).tolist() if num_batches > 0 else [0]*num_classes,\n        'iou': (total_iou / num_batches).tolist() if num_batches > 0 else [0]*num_classes,\n        'precision': (total_precision / num_batches).tolist() if num_batches > 0 else [0]*num_classes,\n        'recall': (total_recall / num_batches).tolist() if num_batches > 0 else [0]*num_classes,\n        'f1_score': (total_f1 / num_batches).tolist() if num_batches > 0 else [0]*num_classes,\n    }\n    return metrics\n\ndef run_and_print_test_evaluation(test_dataloader, device, num_classes):\n    \"\"\"\n    Đánh giá model TỐT NHẤT trên tập test với TTA và in ra các chỉ số metrics.\n    \"\"\"\n    print(\"\\n--- Evaluating on Test Set with TTA ---\")\n    print(\"Khởi tạo kiến trúc model để tải trọng số...\")\n    model = RobustMedVFL_UNet(n_channels=NUM_SLICES, n_classes=num_classes)\n    \n    model_path = \"best_model.pth\"\n    if os.path.exists(model_path):\n        print(f\"Đang tải trọng số của model tốt nhất từ '{model_path}'...\")\n        model.load_state_dict(torch.load(model_path, map_location=device))\n        model.to(device)\n    else:\n        print(f\"Lỗi: Không tìm thấy file model '{model_path}'. Không thể đánh giá.\")\n        return\n        \n    if test_dataloader and test_dataloader.dataset and len(test_dataloader.dataset) > 0:\n        # --- SỬA ĐỔI: Gọi hàm evaluate_metrics_with_tta ---\n        test_metrics = evaluate_metrics_with_tta(model, test_dataloader, device, num_classes)\n        \n        test_accuracy = test_metrics['accuracy']\n        \n        # --- SỬA ĐỔI: Tính trung bình trên tất cả các class ---\n        mean_dice = np.mean(test_metrics['dice_scores'])\n        mean_iou = np.mean(test_metrics['iou'])\n        mean_precision = np.mean(test_metrics['precision'])\n        mean_recall = np.mean(test_metrics['recall'])\n        mean_f1 = np.mean(test_metrics['f1_score'])\n\n        print(f\"\\n  Test Results (Mean of ALL {num_classes} Classes):\")\n        print(f\"    Accuracy: {test_accuracy:.4f}; Dice: {mean_dice:.4f}; IoU: {mean_iou:.4f}; \"\n              f\"Precision: {mean_precision:.4f}; Recall: {mean_recall:.4f}; F1-score: {mean_f1:.4f}\")\n        \n        print(\"\\n  Per-Class Metrics:\")\n        for c_idx in range(num_classes):\n            class_name = ACDC_CLASS_MAP.get(c_idx, f\"Class {c_idx}\")\n            print(f\"    => {class_name:<20}: \"\n                  f\"Dice: {test_metrics['dice_scores'][c_idx]:.4f}, \"\n                  f\"IoU: {test_metrics['iou'][c_idx]:.4f}, \"\n                  f\"Precision: {test_metrics['precision'][c_idx]:.4f}, \"\n                  f\"Recall: {test_metrics['recall'][c_idx]:.4f}, \"\n                  f\"F1: {test_metrics['f1_score'][c_idx]:.4f}\")\n    else:\n        print(\"\\nTest dataset not available or empty. Skipping test evaluation.\")\n\n\ndef visualize_final_results_2_5D(volumes_np, masks_np, num_classes, num_samples, device):\n    \"\"\"\n    Trực quan hóa kết quả bằng cách tự động tải model tốt nhất đã lưu.\n    \"\"\"\n    if not volumes_np:\n        print(\"Không có dữ liệu test để trực quan hóa.\")\n        return\n        \n    print(\"\\n--- Visualizing Final Results ---\")\n    print(\"Khởi tạo kiến trúc model để tải trọng số...\")\n    # Khởi tạo model với đúng số kênh (5 kênh)\n    model = RobustMedVFL_UNet(n_channels=NUM_SLICES, n_classes=num_classes)\n    \n    # Sử dụng đúng tên file đã lưu\n    model_path = \"best_model.pth\"\n    if os.path.exists(model_path):\n        print(f\"Đang tải trọng số của model tốt nhất từ '{model_path}'...\")\n        model.load_state_dict(torch.load(model_path, map_location=device))\n        model.to(device)\n    else:\n        print(f\"Lỗi: Không tìm thấy file model '{model_path}'.\")\n        return\n\n    model.eval()\n    \n    vis_transform = A.Compose([\n        ToTensorV2(),\n    ])\n\n    # Tạo index map để chọn ngẫu nhiên các lát cắt\n    index_map = []\n    for vol_idx, vol in enumerate(volumes_np):\n        for slice_idx in range(vol.shape[2]):\n            index_map.append((vol_idx, slice_idx))\n            \n    if not index_map:\n        print(\"Không có lát cắt nào để hiển thị.\")\n        return\n    sample_indices = random.sample(range(len(index_map)), min(num_samples, len(index_map)))\n\n    # Tạo colormap tùy chỉnh\n    colors = [ACDC_COLOR_MAP.get(i, 'black') for i in range(num_classes)]\n    cmap = mcolors.ListedColormap(colors)\n\n    for i, idx in enumerate(sample_indices):\n        vol_idx, center_slice_idx = index_map[idx]\n        \n        original_image_slice = volumes_np[vol_idx][:, :, center_slice_idx]\n        ground_truth_mask_slice = masks_np[vol_idx][:, :, center_slice_idx]\n        \n        # SỬA LỖI 3: Chuẩn bị input 2.5D với đúng 5 lát cắt\n        current_volume = volumes_np[vol_idx]\n        num_slices_in_vol = current_volume.shape[2]\n        \n        slice_indices_for_stack = []\n        # Lấy 5 lát cắt: offset -2, -1, 0, 1, 2\n        for offset in [-2, -1, 0, 1, 2]:\n            # Dùng np.clip để xử lý các lát cắt ở rìa an toàn hơn\n            slice_idx = np.clip(center_slice_idx + offset, 0, num_slices_in_vol - 1)\n            slice_indices_for_stack.append(slice_idx)\n            \n        image_stack_np = np.stack(\n            [current_volume[:, :, s] for s in slice_indices_for_stack], axis=-1\n        ).astype(np.float32)\n        # Áp dụng transform\n        transformed = vis_transform(image=image_stack_np)\n        model_input = transformed['image'].unsqueeze(0).to(device)\n\n        # Lấy dự đoán từ model đã tải\n        with torch.no_grad():\n            logits_list, _ = model(model_input)\n            logits = logits_list[-1] \n            probabilities = torch.softmax(logits, dim=1)\n            prediction = torch.argmax(probabilities, dim=1).squeeze(0)\n            \n        predicted_mask_slice = prediction.cpu().numpy()\n\n        # Vẽ kết quả\n        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n        fig.suptitle(f'Sample {i+1} (Volume: {vol_idx}, Slice: {center_slice_idx})', fontsize=16)\n        \n        axes[0].imshow(original_image_slice, cmap='gray')\n        axes[0].set_title('Ảnh MRI Gốc')\n        axes[0].axis('off')\n\n        axes[1].imshow(original_image_slice, cmap='gray')\n        pred_masked_display = np.ma.masked_where(predicted_mask_slice == 0, predicted_mask_slice)\n        axes[1].imshow(pred_masked_display, cmap=cmap, alpha=0.6, vmin=0, vmax=num_classes-1)\n        axes[1].set_title('Dự đoán (Model Tốt Nhất)')\n        axes[1].axis('off')\n        \n        axes[2].imshow(original_image_slice, cmap='gray')\n        gt_masked_display = np.ma.masked_where(ground_truth_mask_slice == 0, ground_truth_mask_slice)\n        axes[2].imshow(gt_masked_display, cmap=cmap, alpha=0.6, vmin=0, vmax=num_classes-1)\n        axes[2].set_title('Mặt nạ Ground Truth')\n        axes[2].axis('off')\n\n        legend_elements = [\n            plt.Rectangle((0, 0), 1, 1, color=ACDC_COLOR_MAP[i], label=ACDC_CLASS_MAP[i])\n            for i in range(1, num_classes)\n        ]\n        fig.legend(handles=legend_elements, loc='lower center', ncol=3, bbox_to_anchor=(0.5, -0.02))\n        \n        plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n        plt.show()\n\n    \n# --- CÁCH GỌI HÀM (KHÔNG ĐỔI) ---\n# 1. Chạy đánh giá và in các chỉ số metrics\nrun_and_print_test_evaluation(\n    test_dataloader=test_dataloader,\n    device=DEVICE,\n    num_classes=NUM_CLASSES\n)\n\n# 2. Trực quan hóa kết quả\nvisualize_final_results_2_5D(\n    volumes_np=all_test_volumes,\n    masks_np=all_test_masks,\n    num_classes=NUM_CLASSES,\n    num_samples=50, # Giảm số lượng mẫu để chạy thử nhanh hơn\n    device=DEVICE\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T21:20:00.728945Z","iopub.execute_input":"2025-10-07T21:20:00.729236Z","iopub.status.idle":"2025-10-07T21:20:42.048562Z","shell.execute_reply.started":"2025-10-07T21:20:00.729210Z","shell.execute_reply":"2025-10-07T21:20:42.047754Z"}},"outputs":[],"execution_count":null}]}